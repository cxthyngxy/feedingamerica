## update current theta0, theta1, and J values ##
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
# Store the updated values in vectors for tracking
theta0_values = c(theta0_values, current_theta0)
theta1_values = c(theta1_values, current_theta1)
J_values = c(J_values, current_J)
}
# Final estimated theta values
cat("Final theta0:", current_theta0, "\n")
cat("Final theta1:", current_theta1, "\n")
# Plot theta0 over iterations
plot(1:length(theta0_values), theta0_values, type='l', col='blue', xlab='Iteration',
ylab=expression(theta[0]), main=expression(paste("Values of ", theta[0], " over iterations")))
# Plot theta1 over iterations
plot(1:length(theta1_values), theta1_values, type='l', col='green', xlab='Iteration',
ylab=expression(theta[1]), main=expression(paste("Values of ", theta[1], " over iterations")))
# Plot cost function J over iterations
#plot(1:length(J_values), J_values, type='l', col='red', xlab='Iteration', ylab='Cost  Function J', main='Cost Function J over iterations')
## when computing the cost function value at particular theta0 and theta1, you can use thi
J = function(theta0, theta1, m, x, y) {
yhat= theta0 + theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
## perform gradient descent ##
convg_threshold = 0.001
alpha = .01 # learning rate
# initial guess of theta0 and theta_1
# hint:plot y against rescaled x to pick a good guess of
plot(x, y)
model <- lm(y ~ x)
# View the model summary to get the slope (theta1) and intercept (theta0)
summary(model)
initial_theta0 = 71270.49
initial_theta1 = 134.53 # initial guess of theta1
#initial value of the cost function
# hint: you can compute the cost function value given the initial thetas using the provide
IJ= function(initial_theta0, initial_theta1, m, x, y) {
yhat= initial_theta0 + initial_theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
cost_value <- IJ(initial_theta0, initial_theta1, m, x, y)
print(cost_value)
initial_J = 2058132792
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterati
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterati
J_values = c(current_J)
# a vector to store updated theta1 values during iterati
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) { # hint: this is the condition for the while loop to continue
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, m , x, y )
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, m , x, y ) ## hint: see the previous line
new_J = IJ(new_theta0, new_theta1, m, x, y)
## compute the new cost function
theta0_values = c(theta0_values, new_theta0) # the new theta_0 is stored
theta1_values = c(theta1_values, new_theta1)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, and J values ##
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
# Store the updated values in vectors for tracking
theta0_values = c(theta0_values, current_theta0)
theta1_values = c(theta1_values, current_theta1)
J_values = c(J_values, current_J)
}
## when computing the cost function value at particular theta0 and theta1, you can use thi
J = function(theta0, theta1, m, x, y) {
yhat= theta0 + theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
## perform gradient descent ##
convg_threshold = 0.001
alpha = 0.000000001 # learning rate
# initial guess of theta0 and theta_1
# hint:plot y against rescaled x to pick a good guess of
plot(x, y)
model <- lm(y ~ x)
# View the model summary to get the slope (theta1) and intercept (theta0)
summary(model)
initial_theta0 = 71270.49
initial_theta1 = 134.53 # initial guess of theta1
#initial value of the cost function
# hint: you can compute the cost function value given the initial thetas using the provide
IJ= function(initial_theta0, initial_theta1, m, x, y) {
yhat= initial_theta0 + initial_theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
cost_value <- IJ(initial_theta0, initial_theta1, m, x, y)
print(cost_value)
initial_J = 2058132792
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterati
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterati
J_values = c(current_J)
# a vector to store updated theta1 values during iterati
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) { # hint: this is the condition for the while loop to continue
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, m , x, y )
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, m , x, y ) ## hint: see the previous line
new_J = IJ(new_theta0, new_theta1, m, x, y)
## compute the new cost function
theta0_values = c(theta0_values, new_theta0) # the new theta_0 is stored
theta1_values = c(theta1_values, new_theta1)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, and J values ##
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
# Store the updated values in vectors for tracking
theta0_values = c(theta0_values, current_theta0)
theta1_values = c(theta1_values, current_theta1)
J_values = c(J_values, current_J)
}
# Final estimated theta values
cat("Final theta0:", current_theta0, "\n")
cat("Final theta1:", current_theta1, "\n")
# Plot theta0 over iterations
plot(1:length(theta0_values), theta0_values, type='l', col='blue', xlab='Iteration',
ylab=expression(theta[0]), main=expression(paste("Values of ", theta[0], " over iterations")))
# Plot theta1 over iterations
plot(1:length(theta1_values), theta1_values, type='l', col='green', xlab='Iteration',
ylab=expression(theta[1]), main=expression(paste("Values of ", theta[1], " over iterations")))
# Plot cost function J over iterations
#plot(1:length(J_values), J_values, type='l', col='red', xlab='Iteration', ylab='Cost  Function J', main='Cost Function J over iterations')
# Plot cost function J over iterations
plot(1:length(J_values), J_values, type='l', col='red', xlab='Iteration', ylab='Cost  Function J', main='Cost Function J over iterations')
x <- housing$size
y <- housing$price
# Step 2: Create the design matrix (add a column of ones for the intercept)
X <- cbind(1, x)  # This creates a matrix with a column of ones and the x variable
# Step 3: Compute the parameters using the normal equation
theta <- solve(t(X) %*% X) %*% t(X) %*% y
# Display the estimated parameters
theta0 <- theta[1]  # Intercept
theta1 <- theta[2]  # Slope
cat("Estimated Intercept (theta0):", theta0, "\n")
cat("Estimated Slope (theta1):", theta1, "\n")
## when computing the cost function value at particular theta0 and theta1, you can use thi
J = function(theta0, theta1, m, x, y) {
yhat= theta0 + theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
## perform gradient descent ##
convg_threshold = 0.001
alpha = 0.000000001 # learning rate
# initial guess of theta0 and theta_1
# hint:plot y against rescaled x to pick a good guess of
plot(x, y)
model <- lm(y ~ x)
# View the model summary to get the slope (theta1) and intercept (theta0)
summary(model)
initial_theta0 = 71270
initial_theta1 = 130 # initial guess of theta1
#initial value of the cost function
# hint: you can compute the cost function value given the initial thetas using the provide
IJ= function(initial_theta0, initial_theta1, m, x, y) {
yhat= initial_theta0 + initial_theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
cost_value <- IJ(initial_theta0, initial_theta1, m, x, y)
print(cost_value)
initial_J = 2058132792
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterati
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterati
J_values = c(current_J)
# a vector to store updated theta1 values during iterati
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) { # hint: this is the condition for the while loop to continue
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, m , x, y )
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, m , x, y ) ## hint: see the previous line
new_J = IJ(new_theta0, new_theta1, m, x, y)
## compute the new cost function
theta0_values = c(theta0_values, new_theta0) # the new theta_0 is stored
theta1_values = c(theta1_values, new_theta1)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, and J values ##
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
# Store the updated values in vectors for tracking
theta0_values = c(theta0_values, current_theta0)
theta1_values = c(theta1_values, current_theta1)
J_values = c(J_values, current_J)
}
# Final estimated theta values
cat("Final theta0:", current_theta0, "\n")
cat("Final theta1:", current_theta1, "\n")
# Plot theta0 over iterations
plot(1:length(theta0_values), theta0_values, type='l', col='blue', xlab='Iteration',
ylab=expression(theta[0]), main=expression(paste("Values of ", theta[0], " over iterations")))
# Plot theta1 over iterations
plot(1:length(theta1_values), theta1_values, type='l', col='green', xlab='Iteration',
ylab=expression(theta[1]), main=expression(paste("Values of ", theta[1], " over iterations")))
# Plot cost function J over iterations
plot(1:length(J_values), J_values, type='l', col='red', xlab='Iteration', ylab='Cost  Function J', main='Cost Function J over iterations')
x <- housing$size
y <- housing$price
# Step 2: Create the design matrix (add a column of ones for the intercept)
X <- cbind(1, x)  # This creates a matrix with a column of ones and the x variable
# Step 3: Compute the parameters using the normal equation
theta <- solve(t(X) %*% X) %*% t(X) %*% y
# Display the estimated parameters
theta0 <- theta[1]  # Intercept
theta1 <- theta[2]  # Slope
cat("Estimated Intercept (theta0):", theta0, "\n")
cat("Estimated Slope (theta1):", theta1, "\n")
## when computing the cost function value at particular theta0 and theta1, you can use thi
J = function(theta0, theta1, m, x, y) {
yhat= theta0 + theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
## perform gradient descent ##
convg_threshold = 0.001
alpha = 0.000000001 # learning rate
# initial guess of theta0 and theta_1
# hint:plot y against rescaled x to pick a good guess of
plot(x, y)
model <- lm(y ~ x)
# View the model summary to get the slope (theta1) and intercept (theta0)
summary(model)
initial_theta0 = 71200
initial_theta1 = 100 # initial guess of theta1
#initial value of the cost function
# hint: you can compute the cost function value given the initial thetas using the provide
IJ= function(initial_theta0, initial_theta1, m, x, y) {
yhat= initial_theta0 + initial_theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
cost_value <- IJ(initial_theta0, initial_theta1, m, x, y)
print(cost_value)
initial_J = 2058132792
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterati
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterati
J_values = c(current_J)
# a vector to store updated theta1 values during iterati
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) { # hint: this is the condition for the while loop to continue
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, m , x, y )
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, m , x, y ) ## hint: see the previous line
new_J = IJ(new_theta0, new_theta1, m, x, y)
## compute the new cost function
theta0_values = c(theta0_values, new_theta0) # the new theta_0 is stored
theta1_values = c(theta1_values, new_theta1)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, and J values ##
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
# Store the updated values in vectors for tracking
theta0_values = c(theta0_values, current_theta0)
theta1_values = c(theta1_values, current_theta1)
J_values = c(J_values, current_J)
}
# Final estimated theta values
cat("Final theta0:", current_theta0, "\n")
cat("Final theta1:", current_theta1, "\n")
# Plot theta0 over iterations
plot(1:length(theta0_values), theta0_values, type='l', col='blue', xlab='Iteration',
ylab=expression(theta[0]), main=expression(paste("Values of ", theta[0], " over iterations")))
# Plot theta1 over iterations
plot(1:length(theta1_values), theta1_values, type='l', col='green', xlab='Iteration',
ylab=expression(theta[1]), main=expression(paste("Values of ", theta[1], " over iterations")))
# Plot cost function J over iterations
plot(1:length(J_values), J_values, type='l', col='red', xlab='Iteration', ylab='Cost  Function J', main='Cost Function J over iterations')
## when computing the cost function value at particular theta0 and theta1, you can use thi
J = function(theta0, theta1, m, x, y) {
yhat= theta0 + theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
## perform gradient descent ##
convg_threshold = 0.001
alpha = 0.000000001 # learning rate
# initial guess of theta0 and theta_1
# hint:plot y against rescaled x to pick a good guess of
plot(x, y)
model <- lm(y ~ x)
# View the model summary to get the slope (theta1) and intercept (theta0)
summary(model)
initial_theta0 = 71000
initial_theta1 = 100 # initial guess of theta1
#initial value of the cost function
# hint: you can compute the cost function value given the initial thetas using the provide
IJ= function(initial_theta0, initial_theta1, m, x, y) {
yhat= initial_theta0 + initial_theta1*x
value= (1/(2*m)) * sum((yhat-y)^2)
return(value)
}
cost_value <- IJ(initial_theta0, initial_theta1, m, x, y)
print(cost_value)
initial_J = 2058132792
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterati
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterati
J_values = c(current_J)
# a vector to store updated theta1 values during iterati
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) { # hint: this is the condition for the while loop to continue
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, m , x, y )
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, m , x, y ) ## hint: see the previous line
new_J = IJ(new_theta0, new_theta1, m, x, y)
## compute the new cost function
theta0_values = c(theta0_values, new_theta0) # the new theta_0 is stored
theta1_values = c(theta1_values, new_theta1)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, and J values ##
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
# Store the updated values in vectors for tracking
theta0_values = c(theta0_values, current_theta0)
theta1_values = c(theta1_values, current_theta1)
J_values = c(J_values, current_J)
}
# Final estimated theta values
cat("Final theta0:", current_theta0, "\n")
cat("Final theta1:", current_theta1, "\n")
# Plot theta0 over iterations
plot(1:length(theta0_values), theta0_values, type='l', col='blue', xlab='Iteration',
ylab=expression(theta[0]), main=expression(paste("Values of ", theta[0], " over iterations")))
# Plot theta1 over iterations
plot(1:length(theta1_values), theta1_values, type='l', col='green', xlab='Iteration',
ylab=expression(theta[1]), main=expression(paste("Values of ", theta[1], " over iterations")))
# Plot cost function J over iterations
plot(1:length(J_values), J_values, type='l', col='red', xlab='Iteration', ylab='Cost  Function J', main='Cost Function J over iterations')
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigmaˆ2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, fn = logL, m = m, x = x, y = y)
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigmaˆ2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, logL, x = x, y =y, control = list(fnscale=-1))
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigmaˆ2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
m <- nrow(housing)
result <- optim(par = initial_thetas, logL, x = x, y =y, control = list(fnscale=-1))
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigmaˆ2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, logL, x = x, y =y, m=m, control = list(fnscale=-1))
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigma^2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, logL, x = x, y =y, m=m, control = list(fnscale=-1))
theta0_mle <- result$par[1]
theta1_mle <- result$par[2]
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigma^2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, logL, x = x, y =y, m=m, control = list(fnscale=1))
theta0_mle <- result$par[1]
theta1_mle <- result$par[2]
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigma^2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, logL, x = x, y =y, m=m, control = list(fnscale=-1))
theta0_mle <- result$par[1]
theta1_mle <- result$par[2]
x <- housing$size
y <- housing$price
X <- cbind(1, x)  # create a matrix with a column of ones and the x variable
# Step 3: Compute the parameters using the normal equation
theta_normal <- solve(t(X) %*% X) %*% t(X) %*% y
# Display the estimated parameters
theta0_normale <- theta[1]  # Intercept
theta1_normal <- theta[2]  # Slope
cat("Estimated Intercept (theta0):", theta0, "\n")
cat("Estimated Slope (theta1):", theta1, "\n")
logL = function(thetas, m, x, y) {
sigma=1 # we assume sigma=1
ll=m*log(1/(sqrt(2*pi)*sigma)) - 1/(sigma^2)*(1/2)*sum((thetas[1] + thetas[2]*x - y)^2)
return(-ll)
}
initial_thetas <- c(71000, 100)
result <- optim(par = initial_thetas, logL, x = x, y =y, m=m, control = list(fnscale=-1))
theta0_mle <- result$par[1]
theta1_mle <- result$par[2]
x <- housing$size
y <- housing$price
X <- cbind(1, x)  # create a matrix with a column of ones and the x variable
# Step 3: Compute the parameters using the normal equation
theta_normal <- solve(t(X) %*% X) %*% t(X) %*% y
# Display the estimated parameters
theta0_normal <- theta[1]  # Intercept
theta1_normal <- theta[2]  # Slope
cat("Estimated Intercept (theta0):", theta0, "\n")
cat("Estimated Slope (theta1):", theta1, "\n")
knitr::opts_chunk$set(echo = TRUE)
install.packages("flexdashboard")
knitr::opts_chunk$set(echo = TRUE)
install.packages("flexdashboard")
knitr::opts_chunk$set(echo = TRUE)
install.packages("flexdashboard")
library(flexdashboard)
# Set CRAN mirror explicitly to avoid errors in non-interactive mode
options(repos = c(CRAN = "https://cloud.r-project.org"))
#install.packages("readxl")
install.packages("openxlsx") # If not installed
#library(readxl)
library(openxlsx)
fooddata_counties <- read.xlsx("FeedingAmerica_MMG2024_2019-2022_Data_ToShare_v3.xlsx", sheet = 2)  # By name
fooddata_states <- read.xlsx("FeedingAmerica_MMG2024_2019-2022_Data_ToShare_v3.xlsx", sheet = 4)  # By index
options(repos = c(CRAN = "https://cloud.r-project.org"))
knitr::opts_chunk$set(echo = TRUE)
install.packages("flexdashboard")
library(flexdashboard)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
install.packages("flexdashboard")
options(repos = c(CRAN = "https://cloud.r-project.org"))
knitr::opts_chunk$set(echo = TRUE)
install.packages("flexdashboard")
library(flexdashboard)
ggplotly(state_plot, tooltip = "text")
install.packages("flexdashboard")
ggplotly(state_plot, tooltip = "text")
# Load necessary libraries
library(plotly)
library(dplyr)
ggplotly(state_plot, tooltip = "text")
state_plot <- ggplot(filtered_data) +
aes(
x = Year,
y = Overall.Food.Insecurity.Rate,
colour = State,
group = State.Name,
text = paste(
"Year:", Year,
"<br>State Food Insecurity Rate:", Overall.Food.Insecurity.Rate,
"<br>State:", State.Name
) # Customizing tooltip labels
) +
geom_line(linewidth = 0.95) +
scale_color_brewer(palette = "Paired", direction = 1) +
labs(
x = "Year (2019-2022)",
y = "Food Insecurity Rate (%)",
title = "Food Insecurity Rate (2019-2022) in Top 10 Most Impacted States"
) +
theme_gray()
install.packages("bslib")
library(bslib)
install.packages("bslib")
setwd("~/Downloads/data555_finalproj")
options(repos = c(CRAN = "https://cloud.r-project.org"))
knitr::opts_chunk$set(echo = TRUE)
install.packages("flexdashboard")
library(flexdashboard)
install.packages("bslib")
library(bslib)
# Set CRAN mirror explicitly to avoid errors in non-interactive mode
options(repos = c(CRAN = "https://cloud.r-project.org"))
#install.packages("readxl")
#install.packages("openxlsx") # If not installed
#library(readxl)
library(openxlsx)
fooddata_counties <- read.xlsx("FeedingAmerica_MMG2024_2019-2022_Data_ToShare_v3.xlsx", sheet = 2)  # By name
